{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf8b535-3785-4ed0-9d44-e175be3c8287",
   "metadata": {},
   "source": [
    "# Exploratory: method to extract data from kml file in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0184b65-5bc1-4e1a-8a52-647e06e6c256",
   "metadata": {},
   "source": [
    "In this script we seek to extract refinery data from a google earth keyhole markup language (kml) file and store that in a Python dataframe. Once the data is captured we aim to remove duplicate records and append country, state and city data to each gps coordinate. \n",
    "\n",
    "Our thought is to try and see one part of the oil and gas industry (refineries - downstream) from a country perspective and for larger countries from a state by state perspective. And in so doing attempt to reduce complexity and better understand linkages between states that produce oil and countries that import oil. Further down the track aim to identify those refineries that carry out further processing of refined crude and gas and deliver petrochemical feedstocks such as ethyelene and propylene.\n",
    "\n",
    "The data source is a google earth [link](https://www.google.com/maps/d/u/0/edit?mid=1OyFZs60ZesOe7RZaBwoU6eG4qFMO1Cg&usp=sharing) to a selection of world refineries that has been downloaded and saved in a file called core3.kml. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4088b16-f56c-4c0e-b378-b32a7363701b",
   "metadata": {},
   "source": [
    "## Set up the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64523501-58a4-46b7-be59-2ff316e3f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykml import parser\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54319a6a-f9c0-4041-8ff1-fab0bd0b2831",
   "metadata": {},
   "source": [
    "## Prepare to read in kml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832bbf5d-885f-4401-987a-d5069ec7c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the namespace map to handle the KML namespace\n",
    "ns = {'kml': 'http://www.opengis.net/kml/2.2'}\n",
    "\n",
    "# Open the KML file\n",
    "with open('core3.kml', 'rt', encoding=\"utf-8\") as myfile:\n",
    "    doc = parser.parse(myfile).getroot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33463092-e87f-4c1e-a952-fec25fed3fbb",
   "metadata": {},
   "source": [
    "## Extract data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc2b3694-d588-4f40-9b4f-92c9021aa555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from the KML file\n",
    "data = []\n",
    "for placemark in doc.xpath('.//kml:Placemark', namespaces=ns):\n",
    "    name = placemark.find('kml:name', ns).text if placemark.find('kml:name', ns) is not None else None\n",
    "    description = placemark.find('kml:description', ns).text if placemark.find('kml:description', ns) is not None else None\n",
    "    coordinates_element = placemark.find('.//kml:Point/kml:coordinates', ns)\n",
    "    if coordinates_element is not None:\n",
    "        # Extract coordinates and split by comma\n",
    "        coordinates = coordinates_element.text.strip().split(',')\n",
    "        if len(coordinates) >= 2:\n",
    "            # Take only the first two coordinates (longitude, latitude), reverse them, and ensure they are floats\n",
    "            longitude, latitude = map(float, coordinates[:2])\n",
    "            reversed_coordinates = [latitude, longitude]\n",
    "        else:\n",
    "            reversed_coordinates = None\n",
    "    else:\n",
    "        reversed_coordinates = None\n",
    "    \n",
    "    data.append({'name': name, 'description': description, 'reversed_coordinates': reversed_coordinates})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dca4e6-5cf5-484f-b41d-7296ad07d30b",
   "metadata": {},
   "source": [
    "## Store data in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815d1d6b-edb7-481d-add5-5e3db41e5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the extracted data\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4ea42-b84e-4c84-b9ba-c6a3ad6c32ec",
   "metadata": {},
   "source": [
    "## Tidy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71395565-62ab-4637-9ccc-4be8f3d94351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   name  \\\n",
      "0                             Sonatrach Skikda Refinery   \n",
      "1                              Sonatrach Arzew Refinery   \n",
      "2                     Sonatrach El Harrach Refinery????   \n",
      "3                     Sonatrach Hassi Messaoud Refinery   \n",
      "4                                    NOC Zawia Refinery   \n",
      "...                                                 ...   \n",
      "1817     23 - ExxonMobil Refining + Supply Co. - Altona   \n",
      "1818  24 - Shell Refining (Australia) Pty. Ltd. - Clyde   \n",
      "1819  25 - Shell Refining (Australia) Pty. Ltd. - Ge...   \n",
      "1820  343 - New Zealand Refining Co. Ltd. - Marsden ...   \n",
      "1821                      359 - Interoil - Port Moresby   \n",
      "\n",
      "                                            description  \\\n",
      "0                                           323,000 bpd   \n",
      "1                                            54,000 bpd   \n",
      "2                                            59,000 bpd   \n",
      "3                                            27,000 bpd   \n",
      "4                                           120,000 bpd   \n",
      "...                                                 ...   \n",
      "1817  <img src=\"https://lh3.googleusercontent.com/um...   \n",
      "1818  <img src=\"https://lh3.googleusercontent.com/um...   \n",
      "1819  <img src=\"https://lh3.googleusercontent.com/um...   \n",
      "1820  <img src=\"https://lh3.googleusercontent.com/um...   \n",
      "1821  <img src=\"https://lh3.googleusercontent.com/um...   \n",
      "\n",
      "          reversed_coordinates   latitude   longitude  \n",
      "0        [36.873106, 6.974377]  36.873106    6.974377  \n",
      "1       [35.809656, -0.259589]  35.809656   -0.259589  \n",
      "2        [36.721561, 3.144864]  36.721561    3.144864  \n",
      "3        [31.792197, 6.056817]  31.792197    6.056817  \n",
      "4       [32.786176, 12.696641]  32.786176   12.696641  \n",
      "...                        ...        ...         ...  \n",
      "1817  [-37.850143, 144.850354] -37.850143  144.850354  \n",
      "1818   [-33.828475, 151.03766] -33.828475  151.037660  \n",
      "1819  [-38.077996, 144.381796] -38.077996  144.381796  \n",
      "1820  [-35.843598, 174.493552] -35.843598  174.493552  \n",
      "1821   [-9.455667, 147.114798]  -9.455667  147.114798  \n",
      "\n",
      "[1821 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows where 'reversed_coordinates' is None\n",
    "filtered_df = df.dropna(subset=['reversed_coordinates'])\n",
    "\n",
    "# Create separate columns for latitude and longitude from the filtered DataFrame\n",
    "if not filtered_df.empty:\n",
    "    coordinates_df = pd.DataFrame(filtered_df['reversed_coordinates'].tolist(), columns=['latitude', 'longitude'], index=filtered_df.index)\n",
    "    filtered_df = filtered_df.assign(latitude=coordinates_df['latitude'], longitude=coordinates_df['longitude'])\n",
    "\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d9d9d5-9283-4440-a3c7-c1902741cc12",
   "metadata": {},
   "source": [
    "## Describe data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d980c4e-becb-4415-b06a-af417a17a9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 1821\n",
      "Column count: 5\n",
      "\n",
      "Column Data Types:\n",
      "name                     object\n",
      "description              object\n",
      "reversed_coordinates     object\n",
      "latitude                float64\n",
      "longitude               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Row and Column count\n",
    "print(f\"Row count: {filtered_df.shape[0]}\")\n",
    "print(f\"Column count: {filtered_df.shape[1]}\")\n",
    "\n",
    "# Column Data Types\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(filtered_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd9ad0-dfbc-4121-be96-34812307058f",
   "metadata": {},
   "source": [
    "## Save working data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1386e56f-21af-4cd7-92f2-df62fdcb5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered DataFrame to a excel file\n",
    "filtered_df.to_excel('output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce184a-0907-425a-89c0-57499a89f7c7",
   "metadata": {},
   "source": [
    "## Append country, state and city details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f80d1e-c499-4d1d-85ed-9e92adae5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the geolocator\n",
    "geolocator = Nominatim(user_agent=\"learning_app_getcountry\")\n",
    "\n",
    "def get_location_by_coordinates(lat, lon):\n",
    "    location = geolocator.reverse([lat, lon], exactly_one=True, language=\"en\")\n",
    "    address = location.raw['address']\n",
    "    city = address.get('city', '')\n",
    "    state = address.get('state', '')\n",
    "    country = address.get('country', '')\n",
    "    print(f\"Geocoded coordinates: {lat}, {lon} to {city}, {state}, {country}\")\n",
    "    time.sleep(1)   # Sleep for 1 second to avoid hitting the rate limit       \n",
    "    return city, state, country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd162a-5d75-42d9-9ea1-d6c3ec08a093",
   "metadata": {},
   "source": [
    "## Splitting geocoding into batches for smooth processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6108356-0a40-4542-b33f-9121fed350f2",
   "metadata": {},
   "source": [
    "```python\n",
    "# Convert this markdown block into code to process\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Create a list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Divide the DataFrame into batches\n",
    "for i in np.arange(0, len(filtered_df), batch_size):\n",
    "    batch_df = filtered_df.iloc[i:i+batch_size].copy()\n",
    "    batch_df['city'], batch_df['state'], batch_df['country'] = zip(*batch_df.apply(lambda row: get_location_by_coordinates(row['latitude'], row['longitude']), axis=1))\n",
    "    dfs.append(batch_df)\n",
    "    # Save each batch to a separate Excel file\n",
    "    batch_df.to_excel(f'output_batch_{i//batch_size + 1}.xlsx', index=False)\n",
    "\n",
    "# Merge all batches into a single DataFrame\n",
    "merged_df = pd.concat(dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70276158-b33b-4a80-988a-1c4eea0eae15",
   "metadata": {},
   "source": [
    "## Save the output to different file formats "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e31b9da-06b9-40fe-aeff-f43c21987042",
   "metadata": {},
   "source": [
    "```python\n",
    "# convert this markdown block into code to process\n",
    "\n",
    "# Save the merged DataFrame to an Excel file\r\n",
    "merged_df.to_excel('output_merged.xlsx', index=False)\r\n",
    "\r\n",
    "# Save the merged DataFrame to a pickle file\r\n",
    "merged_df.to_pickle('output_merged.pkl')\r\n",
    "\r\n",
    "# Save the merged DataFrame to a CSV file\r\n",
    "merged_df.to_csv('output_merged.csv', index=False)\r\n",
    "\r\n",
    "# Save the merged DataFrame to a JSON file\r\n",
    "merged_df.to_json('output_merged.json', orient='records')\r\n",
    "\r\n",
    "# Save the merged DataFrame to a HTML file\r\n",
    "merged_df.to_html('output_merged.html', index=False)\r\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
